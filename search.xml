<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[The Simplified SMO Algorithm]]></title>
    <url>%2F05%2F10%2F2020%2F1%2F</url>
    <content type="text"><![CDATA[From Stanford CS229 This document describes a simplified version of the Sequential Minimal Optimization (SMO) algorithm for training support vector machines. Recap of the SVM Optimization Problem Recall from the lecture notes that a support vector machine computes a linear classifier of the form f(x)=wTx+b(1) f(x)=w^{T} x+b \tag{1} f(x)=wTx+b(1) Since we want to apply this to a binary classification problem, we will ultimately predict y = 1 if f(x)&#x2265;0f(x)\ge0f(x)&#x2265;0 and y=&#x2212;1y=&#x2212;1y=&#x2212;1 if f(x)&lt;0f(x)&lt;0f(x)&lt;0, but for now we simply consider the function f(x)f(x)f(x). By looking at the dual problem as we did in Section 6 of the notes, we see that this can also be expressed using inner products as f(x)=&#x2211;i=1m&#x3B1;iy(i)&#x27E8;x(i),x&#x27E9;+b(2) f(x)=\sum_{i=1}^{m} \alpha_{i} y^{(i)}\langle x^{(i)}, x\rangle+b \tag{2} f(x)=i=1&#x2211;m&#x200B;&#x3B1;i&#x200B;y(i)&#x27E8;x(i),x&#x27E9;+b(2) where we can substitute a kernel K(x(i),x)K(x^{(i)}, x)K(x(i),x) in place of the inner product if we so desire. The SMO algorithm gives an efficient way of solving the dual problem of the (regularized) support vector machine optimization problem, given in Section 8 of the notes. Specifically, we wish to solve: max&#x2061;&#x3B1;W(&#x3B1;)=&#x2211;i=1m&#x3B1;i&#x2212;12&#x2211;i=1m&#x2211;j=1my(i)y(j)&#x3B1;i&#x3B1;j&#x27E8;x(i),x(j)&#x27E9;(3) \max_{\alpha} \quad W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j}\langle x^{(i)}, x^{(j)}\rangle \tag{3} &#x3B1;max&#x200B;W(&#x3B1;)=i=1&#x2211;m&#x200B;&#x3B1;i&#x200B;&#x2212;21&#x200B;i=1&#x2211;m&#x200B;j=1&#x2211;m&#x200B;y(i)y(j)&#x3B1;i&#x200B;&#x3B1;j&#x200B;&#x27E8;x(i),x(j)&#x27E9;(3) &#xA0;subject&#xA0;to&#xA0;0&#x2264;&#x3B1;i&#x2264;C,i=1,&#x2026;,m(4) \text { subject to } \quad 0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m \tag{4} &#xA0;subject&#xA0;to&#xA0;0&#x2264;&#x3B1;i&#x200B;&#x2264;C,i=1,&#x2026;,m(4) &#x2211;i=1m&#x3B1;iy(i)=0(5) \sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \tag{5} i=1&#x2211;m&#x200B;&#x3B1;i&#x200B;y(i)=0(5) The KKT conditions can be used to check for convergence to the optimal point. For this problem the KKT conditions are &#x3B1;i=0&#x21D2;y(i)(wTx(i)+b)&#x2265;1(6) \alpha_{i}=0 \quad \Rightarrow \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1 \tag{6} &#x3B1;i&#x200B;=0&#x21D2;y(i)(wTx(i)+b)&#x2265;1(6) &#x3B1;i=C&#x21D2;y(i)(wTx(i)+b)&#x2264;1(7) \alpha_{i}=C \quad \Rightarrow \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \leq 1 \tag{7} &#x3B1;i&#x200B;=C&#x21D2;y(i)(wTx(i)+b)&#x2264;1(7) 0&lt;&#x3B1;i&lt;C&#x21D2;y(i)(wTx(i)+b)=1(8) 0&lt;\alpha_{i}&lt;C \quad \Rightarrow \quad y^{(i)}\left(w^{T} x^{(i)}+b\right)=1 \tag{8} 0&lt;&#x3B1;i&#x200B;&lt;C&#x21D2;y(i)(wTx(i)+b)=1(8) In other words, any &#x3B1;i\alpha_i&#x3B1;i&#x200B;&#x2019;s that satisfy these properties for all i will be an optimal solution to the optimization problem given above. The SMO algorithm iterates until all these conditions are satisfied (to within a certain tolerance) thereby ensuring convergence. The Simplified SMO Algorithm the SMO algorithm selects two &#x3B1; parameters, &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B; and optimizes the objective value jointly for both these &#x3B1;\alpha&#x3B1;&#x2019;s. Finally it adjusts the bbb parameter based on the new &#x3B1;\alpha&#x3B1;&#x2019;s. This process is repeated until the &#x3B1;\alpha&#x3B1;&#x2019;s converge. We now describe these three steps in greater detail. Selecting &#x3B1; Parameters Much of the full SMO algorithm is dedicated to heuristics for choosing which &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B; to optimize so as to maximize the objective function as much as possible. For large data sets, this is critical for the speed of the algorithm, since there are m(m&#x2212;1)m(m&#x2212;1)m(m&#x2212;1) possible choices for &#x3B1;i and &#x3B1;j\alpha_j&#x3B1;j&#x200B;, and some will result in much less improvement than others. However, for our simplified version of SMO, we employ a much simpler heuristic. We simply iterate over all &#x3B1;i,i=1,&#x2026;,m\alpha_i, i = 1,\dots,m&#x3B1;i&#x200B;,i=1,&#x2026;,m. If &#x3B1;i\alpha_i&#x3B1;i&#x200B; does not fulfill the KKT conditions to within some numerical tolerance, we select &#x3B1;j\alpha_j&#x3B1;j&#x200B; at random from the remaining m&#x2212;1m&#x2212;1m&#x2212;1 &#x3B1;\alpha&#x3B1;&#x2019;s and attempt to jointly optimize &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B;. If none of the &#x3B1;\alpha&#x3B1;&#x2019;s are changed after a few iteration over all the &#x3B1;i\alpha_i&#x3B1;i&#x200B;&#x2019;s, then the algorithm terminates. It is important to realize that by employing this simplification, the algorithm is no longer guaranteed to converge to the global optimum (since we are not attempting to optimize all possible &#x3B1;i\alpha_i&#x3B1;i&#x200B;, &#x3B1;j\alpha_j&#x3B1;j&#x200B; pairs, there exists the possibility that some pair could be optimized which we do not consider). Optimizing &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B; Having chosen the Lagrange multipliers &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B; to optimize, we first compute constraints on the values of these parameters, then we solve the constrained maximization problem. First we want to find bounds LLL and HHH such that L&#x2264;&#x3B1;j&#x2264;HL\le\alpha_j\le HL&#x2264;&#x3B1;j&#x200B;&#x2264;H must hold in order for &#x3B1;j\alpha_j&#x3B1;j&#x200B; to satisfy the constraint that 0&#x2264;&#x3B1;j&#x2264;C0\le\alpha_j\le C0&#x2264;&#x3B1;j&#x200B;&#x2264;C. It can be shown that these are given by the following: &#xA0;If&#xA0;y(i)&#x2260;y(j),L=max&#x2061;(0,&#x3B1;j&#x2212;&#x3B1;i),H=min&#x2061;(C,C+&#x3B1;j&#x2212;&#x3B1;i)(10) \text { If } y^{(i)} \neq y^{(j)}, \quad L=\max \left(0, \alpha_{j}-\alpha_{i}\right), \quad H=\min \left(C, C+\alpha_{j}-\alpha_{i}\right) \tag{10} &#xA0;If&#xA0;y(i)&#xE020;&#x200B;=y(j),L=max(0,&#x3B1;j&#x200B;&#x2212;&#x3B1;i&#x200B;),H=min(C,C+&#x3B1;j&#x200B;&#x2212;&#x3B1;i&#x200B;)(10) &#xA0;If&#xA0;y(i)=y(j),L=max&#x2061;(0,&#x3B1;i+&#x3B1;j&#x2212;C),H=min&#x2061;(C,&#x3B1;i+&#x3B1;j)(11) \text { If } y^{(i)}=y^{(j)}, \quad L=\max \left(0, \alpha_{i}+\alpha_{j}-C\right), \quad H=\min \left(C, \alpha_{i}+\alpha_{j}\right) \tag{11} &#xA0;If&#xA0;y(i)=y(j),L=max(0,&#x3B1;i&#x200B;+&#x3B1;j&#x200B;&#x2212;C),H=min(C,&#x3B1;i&#x200B;+&#x3B1;j&#x200B;)(11) Now we want to find &#x3B1;j so as to maximize the objective function. If this value ends up lying outside the bounds LLL and HHH, we simply clip the value of &#x3B1;j\alpha_j&#x3B1;j&#x200B; to lie within this range. It can be shown that the optimal &#x3B1;j is given by: &#x3B1;j:=&#x3B1;j&#x2212;y(j)(Ei&#x2212;Ej)&#x3B7;(12) \alpha_{j}:=\alpha_{j}-\frac{y^{(j)}\left(E_{i}-E_{j}\right)}{\eta} \tag{12} &#x3B1;j&#x200B;:=&#x3B1;j&#x200B;&#x2212;&#x3B7;y(j)(Ei&#x200B;&#x2212;Ej&#x200B;)&#x200B;(12) where Ek=f(x(k))&#x2212;y(k)(13) E_{k}=f\left(x^{(k)}\right)-y^{(k)} \tag{13} Ek&#x200B;=f(x(k))&#x2212;y(k)(13) &#x3B7;=2&#x27E8;x(i),x(j)&#x27E9;&#x2212;&#x27E8;x(i),x(i)&#x27E9;&#x2212;&#x27E8;x(j),x(j)&#x27E9;.(14) \eta=2\langle x^{(i)}, x^{(j)}\rangle-\langle x^{(i)}, x^{(i)}\rangle-\langle x^{(j)}, x^{(j)}\rangle. \tag{14} &#x3B7;=2&#x27E8;x(i),x(j)&#x27E9;&#x2212;&#x27E8;x(i),x(i)&#x27E9;&#x2212;&#x27E8;x(j),x(j)&#x27E9;.(14) You can think of EkE_kEk&#x200B; as the error between the SVM output on the kth example and the true label y(k)y^{(k)}y(k). This can be calculated using equation (2). When calculating the &#x3B7;\eta&#x3B7; parameter you can use a kernel function KKK in place of the inner product if desired. Next we clip &#x3B1;j\alpha_j&#x3B1;j&#x200B; to lie within the range [L,H][L, H][L,H] &#x3B1;j:={H&#xA0;if&#xA0;&#x3B1;j&gt;H&#x3B1;j&#xA0;if&#xA0;L&#x2264;&#x3B1;j&#x2264;HL&#xA0;if&#xA0;&#x3B1;j&lt;L.(15) \alpha_{j}:=\left\{\begin{array}{ll} H &amp; \text { if } \alpha_{j}&gt;H \\ \alpha_{j} &amp; \text { if } L \leq \alpha_{j} \leq H \\ L &amp; \text { if } \alpha_{j}&lt;L. \end{array}\right. \tag{15} &#x3B1;j&#x200B;:=&#x23A9;&#x23A8;&#x23A7;&#x200B;H&#x3B1;j&#x200B;L&#x200B;&#xA0;if&#xA0;&#x3B1;j&#x200B;&gt;H&#xA0;if&#xA0;L&#x2264;&#x3B1;j&#x200B;&#x2264;H&#xA0;if&#xA0;&#x3B1;j&#x200B;&lt;L.&#x200B;(15) Finally, having solved for &#x3B1;j\alpha_j&#x3B1;j&#x200B; we want to find the value for &#x3B1;i\alpha_i&#x3B1;i&#x200B;. This is given by &#x3B1;i:=&#x3B1;i+y(i)y(j)(&#x3B1;j(old)&#x2212;&#x3B1;j)(16) \alpha_{i}:=\alpha_{i}+y^{(i)} y^{(j)}\left(\alpha_{j}^{(\mathrm{old})}-\alpha_{j}\right) \tag{16} &#x3B1;i&#x200B;:=&#x3B1;i&#x200B;+y(i)y(j)(&#x3B1;j(old)&#x200B;&#x2212;&#x3B1;j&#x200B;)(16) where &#x3B1;j(old)\alpha^{(old)}_j&#x3B1;j(old)&#x200B; is the value of &#x3B1;j\alpha_j&#x3B1;j&#x200B; before optimization by (12) and (15). The full SMO algorithm can also handle the rare case that &#x3B7;=0\eta=0&#x3B7;=0. For our purposes, if &#x3B7;=0\eta=0&#x3B7;=0, you can treat this as a case where we cannot make progress on this pair of &#x3B1;\alpha&#x3B1;&#x2019;s. Computing the bbb threshold After optimizing &#x3B1;i\alpha_i&#x3B1;i&#x200B; and &#x3B1;j\alpha_j&#x3B1;j&#x200B;, we select the threshold b such that the KKT conditions are satisfied for the iiith and jjjth examples. If, after optimization, &#x3B1;i\alpha_i&#x3B1;i&#x200B; is not at the bounds (i.e., 0&lt;&#x3B1;i&lt;C0&lt;\alpha_i&lt;C0&lt;&#x3B1;i&#x200B;&lt;C), then the following threshold b1 is valid, since it forces the SVM to output y(i)y^{(i)}y(i) when the input is x(i)x^{(i)}x(i) b1=b&#x2212;Ei&#x2212;y(i)(&#x3B1;i&#x2212;&#x3B1;i(old))&#x27E8;x(i),x(i)&#x27E9;&#x2212;y(j)(&#x3B1;j&#x2212;&#x3B1;j(old))&#x27E8;x(i),x(j)&#x27E9;(17) b_{1}=b-E_{i}-y^{(i)}\left(\alpha_{i}-\alpha_{i}^{(\mathrm{old})}\right)\langle x^{(i)}, x^{(i)}\rangle-y^{(j)}\left(\alpha_{j}-\alpha_{j}^{(\mathrm{old})}\right)\langle x^{(i)}, x^{(j)}\rangle \tag{17} b1&#x200B;=b&#x2212;Ei&#x200B;&#x2212;y(i)(&#x3B1;i&#x200B;&#x2212;&#x3B1;i(old)&#x200B;)&#x27E8;x(i),x(i)&#x27E9;&#x2212;y(j)(&#x3B1;j&#x200B;&#x2212;&#x3B1;j(old)&#x200B;)&#x27E8;x(i),x(j)&#x27E9;(17) Similarly, the following threshold b2b_2b2&#x200B; is valid if 0&lt;&#x3B1;j&lt;C0&lt;\alpha_j&lt;C0&lt;&#x3B1;j&#x200B;&lt;C b2=b&#x2212;Ej&#x2212;y(i)(&#x3B1;i&#x2212;&#x3B1;i(old))&#x27E8;x(i),x(j)&#x27E9;&#x2212;y(j)(&#x3B1;j&#x2212;&#x3B1;j(old))&#x27E8;x(j),x(j)&#x27E9;(18) b_{2}=b-E_{j}-y^{(i)}\left(\alpha_{i}-\alpha_{i}^{(\mathrm{old})}\right)\langle x^{(i)}, x^{(j)}\rangle-y^{(j)}\left(\alpha_{j}-\alpha_{j}^{(\mathrm{old})}\right)\langle x^{(j)}, x^{(j)}\rangle \tag{18} b2&#x200B;=b&#x2212;Ej&#x200B;&#x2212;y(i)(&#x3B1;i&#x200B;&#x2212;&#x3B1;i(old)&#x200B;)&#x27E8;x(i),x(j)&#x27E9;&#x2212;y(j)(&#x3B1;j&#x200B;&#x2212;&#x3B1;j(old)&#x200B;)&#x27E8;x(j),x(j)&#x27E9;(18) If both 0&lt;&#x3B1;i&lt;C0&lt;\alpha_i&lt;C0&lt;&#x3B1;i&#x200B;&lt;C and 0&lt;&#x3B1;j&lt;C0&lt;\alpha_j&lt;C0&lt;&#x3B1;j&#x200B;&lt;C then both these thresholds are valid, and they will be equal. If both new &#x3B1;\alpha&#x3B1;&#x2019;s are at the bounds (i.e., &#x3B1;i=0\alpha_i=0&#x3B1;i&#x200B;=0 or &#x3B1;i=C\alpha_i=C&#x3B1;i&#x200B;=C and &#x3B1;j=0\alpha_j=0&#x3B1;j&#x200B;=0 or &#x3B1;j=C\alpha_j=C&#x3B1;j&#x200B;=C) then all the thresholds between b1 and b2 satisfy the KKT conditions, we we let b:=(b1+b2)/2b := (b1 + b2)/2b:=(b1+b2)/2. This gives the complete equation for bbb, b:={b1&#xA0;if&#xA0;0&lt;&#x3B1;i&lt;Cb2&#xA0;if&#xA0;0&lt;&#x3B1;j&lt;C(b1+b2)/2&#xA0;otherwise&#xA0;(19) b:=\left\{\begin{array}{ll} b_{1} &amp; \text { if } 0&lt;\alpha_{i}&lt;C \\ b_{2} &amp; \text { if } 0&lt;\alpha_{j}&lt;C \\ \left(b_{1}+b_{2}\right) / 2 &amp; \text { otherwise } \end{array}\right. \tag{19} b:=&#x23A9;&#x23A8;&#x23A7;&#x200B;b1&#x200B;b2&#x200B;(b1&#x200B;+b2&#x200B;)/2&#x200B;&#xA0;if&#xA0;0&lt;&#x3B1;i&#x200B;&lt;C&#xA0;if&#xA0;0&lt;&#x3B1;j&#x200B;&lt;C&#xA0;otherwise&#xA0;&#x200B;(19)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gaussian Discriminant Analysis Model]]></title>
    <url>%2F05%2F04%2F2020%2F1%2F</url>
    <content type="text"><![CDATA[From Stanford CS229 When we have a classification problem in which the input features xxx are continuous-valued random variables, we can then use the Gaussian Discriminant Analysis (GDA) model, which models p(x&#x2223;y)p(x|y)p(x&#x2223;y) using a multivariate normal distribution. The model is: y&#x223C;Bernoulli(&#x3D5;)x&#x2223;y=0&#x223C;N(&#x3BC;0,&#x3A3;)x&#x2223;y=1&#x223C;N(&#x3BC;1,&#x3A3;) \begin{aligned} y&amp;\sim\text{Bernoulli}(\phi)\\ x|y=0&amp;\sim\mathcal{N}(\mu_0,\Sigma)\\ x|y=1&amp;\sim\mathcal{N}(\mu_1,\Sigma) \end{aligned} yx&#x2223;y=0x&#x2223;y=1&#x200B;&#x223C;Bernoulli(&#x3D5;)&#x223C;N(&#x3BC;0&#x200B;,&#x3A3;)&#x223C;N(&#x3BC;1&#x200B;,&#x3A3;)&#x200B; Writing out the distribution, this is: p(y)=&#x3D5;y(1&#x2212;&#x3D5;)1&#x2212;yp(x&#x2223;y=0)=1(2&#x3C0;)n/2&#x2223;&#x3A3;&#x2223;1/2exp&#x2061;(&#x2212;12(x&#x2212;&#x3BC;0)T&#x3A3;&#x2212;1(x&#x2212;&#x3BC;0))p(x&#x2223;y=1)=1(2&#x3C0;)n/2&#x2223;&#x3A3;&#x2223;1/2exp&#x2061;(&#x2212;12(x&#x2212;&#x3BC;1)T&#x3A3;&#x2212;1(x&#x2212;&#x3BC;1)) \begin{aligned} p(y)&amp;=\phi^y(1-\phi)^{1-y}\\ p(x|y=0)&amp;=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)\\ p(x|y=1)&amp;=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right) \end{aligned} p(y)p(x&#x2223;y=0)p(x&#x2223;y=1)&#x200B;=&#x3D5;y(1&#x2212;&#x3D5;)1&#x2212;y=(2&#x3C0;)n/2&#x2223;&#x3A3;&#x2223;1/21&#x200B;exp(&#x2212;21&#x200B;(x&#x2212;&#x3BC;0&#x200B;)T&#x3A3;&#x2212;1(x&#x2212;&#x3BC;0&#x200B;))=(2&#x3C0;)n/2&#x2223;&#x3A3;&#x2223;1/21&#x200B;exp(&#x2212;21&#x200B;(x&#x2212;&#x3BC;1&#x200B;)T&#x3A3;&#x2212;1(x&#x2212;&#x3BC;1&#x200B;))&#x200B; Here, the parameters of our model are &#x3D5;\phi&#x3D5;, &#x3A3;\Sigma&#x3A3;, &#x3BC;0\mu_0&#x3BC;0&#x200B;, and &#x3BC;1\mu_1&#x3BC;1&#x200B;. (Note this model is usually applied using only one covariance matrix &#x3A3;\Sigma&#x3A3;.) The log-likelihood of the data is given by &#x2113;(&#x3D5;,&#x3BC;0,&#x3BC;1,&#x3A3;)=log&#x2061;&#x220F;i=1mp(x(i),y(i);&#x3D5;,&#x3BC;0,&#x3BC;1,&#x3A3;)=log&#x2061;&#x220F;i=1mp(x(i)&#x2223;y(i);&#x3BC;0,&#x3BC;1,&#x3A3;)p(y(i);&#x3D5;). \begin{aligned} \ell(\phi,\mu_0,\mu_1,\Sigma) &amp;=\log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\ &amp;=\log\prod_{i=1}^mp(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi). \end{aligned} &#x2113;(&#x3D5;,&#x3BC;0&#x200B;,&#x3BC;1&#x200B;,&#x3A3;)&#x200B;=logi=1&#x220F;m&#x200B;p(x(i),y(i);&#x3D5;,&#x3BC;0&#x200B;,&#x3BC;1&#x200B;,&#x3A3;)=logi=1&#x220F;m&#x200B;p(x(i)&#x2223;y(i);&#x3BC;0&#x200B;,&#x3BC;1&#x200B;,&#x3A3;)p(y(i);&#x3D5;).&#x200B; By maximizing &#x2113;\ell&#x2113; with respect to the parameters, we find the maximum likelihood estimate of the parameters to be: &#x3D5;=1m&#x2211;i=1m1{y(i)=1}&#x3BC;0=&#x2211;i=1m1{y(i)=0}x(i)&#x2211;i=1m1{y(i)=0}&#x3BC;1=&#x2211;i=1m1{y(i)=1}x(i)&#x2211;i=1m1{y(i)=1}&#x3A3;=1m&#x2211;i=1m(x(i)&#x2212;&#x3BC;y(i))(x(i)&#x2212;&#x3BC;y(i))T \begin{aligned} \phi&amp;=\frac{1}{m}\sum_{i=1}^m\bold{1}\{y^{(i)}=1\}\\ \mu_0&amp;=\frac{\sum_{i=1}^m\bold{1}\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m\bold{1}\{y^{(i)}=0\}}\\ \mu_1&amp;=\frac{\sum_{i=1}^m\bold{1}\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m\bold{1}\{y^{(i)}=1\}}\\ \Sigma&amp;=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T \end{aligned} &#x3D5;&#x3BC;0&#x200B;&#x3BC;1&#x200B;&#x3A3;&#x200B;=m1&#x200B;i=1&#x2211;m&#x200B;1{y(i)=1}=&#x2211;i=1m&#x200B;1{y(i)=0}&#x2211;i=1m&#x200B;1{y(i)=0}x(i)&#x200B;=&#x2211;i=1m&#x200B;1{y(i)=1}&#x2211;i=1m&#x200B;1{y(i)=1}x(i)&#x200B;=m1&#x200B;i=1&#x2211;m&#x200B;(x(i)&#x2212;&#x3BC;y(i)&#x200B;)(x(i)&#x2212;&#x3BC;y(i)&#x200B;)T&#x200B; Shown in the &#xFB01;gure are the training set, as well as the contours of the two Gaussian distributions that have been fit to the data in each of the two classes. Note that the two Gaussians have contours that are the same shape and orientation, since they share a covariance matrix &#x3A3;\Sigma&#x3A3;, but they have di&#xFB00;erent means &#x3BC;0\mu_0&#x3BC;0&#x200B; and &#x3BC;1\mu_1&#x3BC;1&#x200B;. Also shown in the figure is the straight line giving the decision boundary at which p(y=1&#x2223;x)=0.5p(y=1|x)=0.5p(y=1&#x2223;x)=0.5. On one side of the boundary, we&#x2019;ll predict y=1y=1y=1 to be the most likely outcome, and on the other side, we&#x2019;ll predict y=0y=0y=0. To summarize: GDA makes stronger modeling assumptions, and is more data e&#xFB03;cient (i.e., requires less training data to learn &#x201C;well&#x201D;) when the modeling assumptions are correct or at least approximately correct. Logistic regression makes weaker assumptions, and is signi&#xFB01;cantly more robust to deviations from modeling assumptions. Speci&#xFB01;cally, when the data is indeed non-Gaussian, then in the limit of large datasets, logistic regression will almost always do better than GDA. For this reason, in practice logistic regression is used more often than GDA. (Some related considerations about discriminative vs. generative models also apply for the Naive Bayes algorithm that we discuss next, but the Naive Bayes algorithm is still considered a very good, and is certainly also a very popular, classi&#xFB01;cation algorithm.)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>GDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install the Latest Nginx on Ubuntu]]></title>
    <url>%2F05%2F04%2F2020%2F1%2F</url>
    <content type="text"><![CDATA[First, install necessary tools with the command: 1sudo apt-get install curl gnupg2 ca-certificates lsb-release There is an official NGINX repository where the latest version can be found. To add this repository, create a new .list file with the command: 1echo &quot;deb http://nginx.org/packages/mainline/ubuntu `lsb_release -cs` nginx&quot; | tee /etc/apt/sources.list.d/nginx.list Before installation, the NGINX public key must be added. To do this, issue the following commands: 1cd /tmp/ &amp;&amp; curl -fsSL https://nginx.org/keys/nginx_signing.key | apt-key add - Once you have the key installed, update apt with the command: 1apt-get update Now it&apos;s time to install the latest version of NGINX. Do so with the command: 1apt-get install nginx Link your config file with the command: 1ln -s /var/www/website/config/my_nginx.conf /etc/nginx/conf.d/]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Monte Carlo Tree Search]]></title>
    <url>%2F05%2F25%2F2019%2F1%2F</url>
    <content type="text"><![CDATA[What is MCTS? Monte Carlo Tree Search (MCTS) is a method for making optimal decisions in artificial intelligence (AI) problems, typically move planning in combinatorial games. It combines the generality of random simulation with the precision of tree search. Research interest in MCTS has risen sharply due to its spectacular success with computer Go and potential application to a number of other difficult problems. Its application extends beyond games, and MCTS can theoretically be applied to any domain that can be described in terms of {state, action} pairs and simulation used to forecast outcomes. Basic Algorithm The basic MCTS algorithm is simple: a search tree is built, node by node, according to the outcomes of simulated playouts. The process can be broken down into the following steps. 1. Selection Starting at root node R, recursively select optimal child nodes (explained below) until a leaf node L is reached. 2. Expansion If L is a not a terminal node (i.e. it does not end the game) then create one or more child nodes and select one C. 3. Simulation Run a simulated playout from C until a result is achieved. 4. Backpropagation Update the current move sequence with the simulation result. Each node must contain two important pieces of information: an estimated value based on simulation results and the number of times it has been visited. In its simplest and most memory efficient implementation, MCTS will add one child node per iteration. Note, however, that it may be beneficial to add more than one child node per iteration depending on the application. Node Selection Bandits and UCB Node selection during tree descent is achieved by choosing the node that maximises some quantity, analogous to the multiarmed bandit problem in which a player must choose the slot machine (bandit) that maximises the estimated reward each turn. An Upper Confidence Bounds (UCB) formula of the following form is typically used: vi+C&#xD7;lnNni v_i+C\times\sqrt{\frac{lnN}{n_i}} vi&#x200B;+C&#xD7;ni&#x200B;lnN&#x200B;&#x200B; where viv_ivi&#x200B; is the estimated value of the node, nin_ini&#x200B; is the number of the times the node has been visited and NNN is the total number of times that its parent has been visited. CCC is a tunable bias parameter. Exploitation vs Exploration The UCB formula balances the exploitation of known rewards with the exploration of relatively unvisited nodes to encourage their exercise. Reward estimates are based on random simulations, so nodes must be visited a number of times before these estimates become reliable; MCTS estimates will typically be unreliable at the start of a search but converge to more reliable estimates given sufficient time and perfect estimates given infinite time. MCTS and UCT Kocsis and Szepervari (2006) first formalised a complete MCTS algorithm by extending UCB to minimax tree search and named it the Upper Confidence Bounds for Trees (UCT) method. This is the algorithm used in the vast majority of current MCTS implementations. UCT may be described as a special case of MCTS, that is: UCT = MCTS + UCB. Benefits MCTS offers a number of advantages over traditional tree search methods. Aheuristic MCTS does not require any strategic or tactical knowledge about the given domain to make reasonable decisions. The algorithm can function effectively with no knowledge of a game apart from its legal moves and end conditions; this means that a single MCTS implementation can be reused for a number of games with little modification, and makes MCTS a potential boon for general game playing. Asymmetric MCTS performs asymmetric tree growth that adapts to the topology of the search space. The algorithm visits more interesting nodes more often, and focusses its search time in more relevant parts of the tree. This makes MCTS suitable for games with large branching factors such as 19x19 Go. Such large combinatorial spaces typically cause problems for standard depth- or breadth-based search methods, but the adaptive nature of MCTS means that it will (eventually) find those moves that appear optimal and focus its search effort there. Anytime The algorithm can be halted at any time to return the current best estimate. The search tree built thus far may be discarded or preserved for future reuse. Drawbacks MCTS has few drawbacks, but they can be major. Playing Strength The MCTS algorithm, in its basic form, can fail to find reasonable moves for even games of medium complexity within a reasonable amount of time. This is mostly due to the sheer size of the combinatorial move space and the fact that key nodes may not be visited enough times to give reliable estimates. Speed MCTS search can take many iterations to converge to a good solution, which can be an issue for more general applications that are difficult to optimise. For example, the best Go implementations can require millions of playouts in conjunction with domain specific optimisations and enhancements to make expert moves, whereas the best GGP implementations may only make tens of (domain independent) playouts per second for more complex games. For reasonable move times, such GGPs may barely have time to visit each legal move and it is unlikely that significant search will occur. Luckily, the performance of the algorithm can be sigificantly improved using a number of techniques. Improvements Dozens of MCTS enhancements have been suggested to date. These can generally be described as being either domain knowledge or domain independent. Domain Knowledge Domain knowledge specific to the current game can be exploited in the tree to filter out implausible moves or in the simulations to produce heavy playouts that are more similar to playouts that would occur between human opponents. This means that playout results will be more realistic than random simulations and that nodes will require fewer iterations to yield realistic reward values. Domain knowledge can yield significant improvements, at the expense of speed and loss of generality. Domain Independent Domain independent enhancements apply to all problem domains. These are typically applied in the tree (e.g. AMAF) although again some apply to the simulations (e.g. prefer winning moves during playouts). Domain independent enhancements do not tie the implementation to a particular domain, maintaining generality, and are hence the focus of most current work in the area.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MCTS</tag>
      </tags>
  </entry>
</search>
